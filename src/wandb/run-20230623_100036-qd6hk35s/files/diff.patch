diff --git a/config_files/htune/zaidnet_big__toy_aes.json b/config_files/htune/zaidnet_big__toy_aes.json
index 9129da3..c83e475 100644
--- a/config_files/htune/zaidnet_big__toy_aes.json
+++ b/config_files/htune/zaidnet_big__toy_aes.json
@@ -21,14 +21,15 @@
     "val_split_prop": 0.1,
     "training_metrics": {"accuracy": "get_acc", "rank": "get_rank"},
     "eval_metrics": {"accuracy": "get_acc", "rank": "get_rank"},
-    "selection_metric": "acc",
+    "selection_metric": "accuracy",
     "maximize_selection_metric": true,
+    "wandb_project": "htune__zaidnet_big__toy_aes",
     "wandb_config": {
         "method": "random",
         "name": "zaidnet_big",
         "parameters": {
-            "scheduler_kwargs": {"max_lr": {"distribution": "log_uniform_values", "min": 1e-4, "max": 1e-2}},
-            "pooling_method": {"values": ["aa_sconv", "aa_maxpool", "aa", "avgpool", "none"]}
+            "lr_scheduler_kwargs": {"max_lr": {"distribution": "log_uniform_values", "min": 1e-4, "max": 1e-2}},
+            "model_kwargs": {"pooling_method": {"values": ["aa_sconv", "aa_maxpool", "aa", "avgpool"]}}
         }
     }
 }
\ No newline at end of file
diff --git a/src/config/__init__.py b/src/config/__init__.py
index feb7903..47e5508 100644
--- a/src/config/__init__.py
+++ b/src/config/__init__.py
@@ -51,7 +51,7 @@ def load_config(config, train=True):
 # Remove all nested dictionaries and concatenate nested keys, i.e. {key1: {key2: val}} becomes {key1-key2: val}.
 #   Necessary because the WandB hyperparameter tuners can't handle nested dictionaries.
 def denest_dict(d, delim='-'):
-    if any(delim in key for k in d.keys()):
+    if any(delim in key for key in d.keys()):
         raise Exception('Delimiter character is used in one or more keys: \'{}\''.format(
             delim, '\', \''.join(list(d.keys()))
         ))
diff --git a/src/main.py b/src/main.py
index 407c824..ec69cb3 100644
--- a/src/main.py
+++ b/src/main.py
@@ -54,7 +54,8 @@ def run_wandb_trial_(device, classifier_settings):
                 classifier_settings[wc_key].update(wc_val)
             else:
                 classifier_settings[wc_key] = wc_val
-        save_dir = config.results_subdir(settings['save_dir'])
+        save_dir = os.path.join(config.RESULTS_BASE_DIR, classifier_settings['save_dir'])
+        os.makedirs(save_dir, exist_ok=True)
         if len(os.listdir(save_dir)) > 0:
             save_dir = os.path.join(save_dir, 'trial_%d'%(max(int(f.split('_')[-1]) for f in os.listdir(save_dir))+1))
         else:
@@ -62,16 +63,16 @@ def run_wandb_trial_(device, classifier_settings):
         results_save_dir = os.path.join(save_dir, 'results')
         model_save_dir = os.path.join(save_dir, 'models')
         figures_save_dir = os.path.join(save_dir, 'figures')
-        os.makedirs(save_dir, exist_ok=True)
         os.makedirs(results_save_dir, exist_ok=True)
         os.makedirs(model_save_dir, exist_ok=True)
         os.makedirs(figures_save_dir, exist_ok=True)
         config.specify_log_file(os.path.join(save_dir, 'log.txt'))
         with open(os.path.join(save_dir, 'settings.json'), 'w') as F:
-            json.dump(settings, F, indent=2)
+            json.dump(classifier_settings, F, indent=2)
         trainer = ClassifierTrainer(using_wandb=True, **classifier_settings)
-        trainer.train_model(settings['total_epochs'], results_save_dir=results_save_dir, model_save_dir=model_save_dir)
+        trainer.train_model(classifier_settings['total_epochs'], results_save_dir=results_save_dir, model_save_dir=model_save_dir)
     except Exception:
+        traceback.print_exc()
         with open(os.path.join(save_dir, 'log.txt'), 'a') as F:
             traceback.print_exc(file=F)
         raise Exception('Trial code crashed.')
@@ -85,10 +86,10 @@ def htune_run(settings, trials_per_gpu=1, devices='cpu'):
     if 'sweep_id' in settings:
         sweep_id = settings['sweep_id']
     else:
-        sweep_id = wandb.sweep(sweep=wandb.config, project=settings['wandb_project'])
+        sweep_id = wandb.sweep(sweep=wandb_config, project=settings['wandb_project'])
     config.set_num_agents(trials_per_gpu*len(devices))
     if trials_per_gpu*len(devices) == 1:
-        spawn_agent(sweep_id, devices[0], classifier_settings)
+        spawn_agent_(sweep_id, settings['wandb_project'], devices[0], classifier_settings)
     else:
         procs = []
         for dev_idx, dev in enumerate(devices):
diff --git a/src/models/classifiers/big_zaidnet.py b/src/models/classifiers/big_zaidnet.py
index b6628cc..d1fe21f 100644
--- a/src/models/classifiers/big_zaidnet.py
+++ b/src/models/classifiers/big_zaidnet.py
@@ -66,7 +66,9 @@ class FEBlock(VerboseModule):
             self.conv = nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, stride=1, padding=kernel_size//2)
         self.selu = nn.SELU()
         self.bn = nn.BatchNorm1d(out_channels, eps=1e-3, momentum=0.9)
-        if pooling_method == 'aa_maxpool':
+        if pooling_method == 'aa_sconv':
+            pass
+        elif pooling_method == 'aa_maxpool':
             self.pool = nn.Sequential(
                 nn.MaxPool1d(5, stride=1),
                 BlurPool(out_channels, filt_size=kernel_size, stride=5)
@@ -115,7 +117,7 @@ class BigZaidNet(VerboseModule):
         
         initial_channels = mlp_dims // 2**(fe_blocks-1)
         self.feature_extractor = nn.Sequential(
-            FEBlock(input_shape[0], initial_channels, 11, pooling_method='none'),
+            FEBlock(input_shape[0], initial_channels, 11, pooling_method='none' if pooling_method == 'aa_sconv' else pooling_method),
             *[FEBlock(initial_channels*2**n, initial_channels*2**(n+1), 11, pooling_method=pooling_method)
               for n in range(fe_blocks-1)]
         )
diff --git a/src/train/classifier.py b/src/train/classifier.py
index be45e00..9cd8cad 100644
--- a/src/train/classifier.py
+++ b/src/train/classifier.py
@@ -4,6 +4,7 @@ import time
 from copy import copy
 import pickle
 from matplotlib import pyplot as plt
+import wandb
 import numpy as np
 import torch
 from torch import nn, optim
