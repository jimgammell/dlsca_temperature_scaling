
Loading data into memory ...
















































100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:36<00:00,  2.64it/s]
Loading data into memory ...
















































100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:36<00:00,  2.65it/s]
Traceback (most recent call last):
  File "/local/a/jgammell/dlsca_bag_of_tricks/src/main.py", line 73, in run_wandb_trial_
    trainer.train_model(classifier_settings['total_epochs'], results_save_dir=results_save_dir, model_save_dir=model_save_dir)
  File "/local/a/jgammell/dlsca_bag_of_tricks/src/train/classifier.py", line 237, in train_model
    self.reset(epochs)
  File "/local/a/jgammell/dlsca_bag_of_tricks/src/train/classifier.py", line 121, in reset
    self.lr_scheduler = getattr(optim.lr_scheduler, self.lr_scheduler_class)(self.optimizer, **self.lr_scheduler_kwargs)
TypeError: __init__() missing 1 required positional argument: 'max_lr'