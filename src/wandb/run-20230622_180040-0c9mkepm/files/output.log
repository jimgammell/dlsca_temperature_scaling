
Loading data into memory ...














































100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:33<00:00,  2.72it/s]
  2%|██▌                                                                                                                                   | 5/256 [00:01<01:33,  2.69it/s]















































100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:34<00:00,  2.72it/s]
Traceback (most recent call last):
  File "/local/a/jgammell/dlsca_bag_of_tricks/src/main.py", line 73, in run_wandb_trial_
    trainer.train_model(classifier_settings['total_epochs'], results_save_dir=results_save_dir, model_save_dir=model_save_dir)
  File "/local/a/jgammell/dlsca_bag_of_tricks/src/train/classifier.py", line 237, in train_model
    self.reset(epochs)
  File "/local/a/jgammell/dlsca_bag_of_tricks/src/train/classifier.py", line 121, in reset
    self.lr_scheduler = getattr(optim.lr_scheduler, self.lr_scheduler_class)(self.optimizer, **self.lr_scheduler_kwargs)
TypeError: __init__() missing 1 required positional argument: 'max_lr'